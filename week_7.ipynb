{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mother', 'died', 'today.', 'Or', 'maybe,', 'yesterday;', 'I', \"can't\", 'be', 'sure']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"I am an invisible man\",\n",
    "          \"The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move\",\n",
    "          \"Mother died today. Or maybe, yesterday; I can't be sure\",\n",
    "          \"It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn’t know what I was doing in New York\",\n",
    "          \"Ships at a distance have every man’s wish on board\",\n",
    "          \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold\",\n",
    "          \"It was a bright cold day in April, and the clocks were striking thirteen\",\n",
    "          \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\"]\n",
    "\n",
    "document = corpus[2]\n",
    "print(document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'a', 'queer', ',', 'sultry', 'summer', ',', 'the', 'summer', 'they', 'electrocuted', 'the', 'Rosenbergs', ',', 'and', 'I', 'didn', '’', 't', 'know', 'what', 'I', 'was', 'doing', 'in', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "\n",
    "# download the most recent punkt package\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "document = corpus[3]\n",
    "print(nltk.tokenize.word_tokenize(document, language='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\n",
    "\n",
    "\n",
    "def is_punct(string):\n",
    "    \"\"\"Check if STRING is a punctuation marker or a sequence of\n",
    "       punctuation markers.\n",
    "\n",
    "    Arguments:\n",
    "        string (str): a string to check for punctuation markers.\n",
    "\n",
    "    Returns:\n",
    "        bool: True is string is a (sequence of) punctuation marker(s),\n",
    "            False otherwise.\n",
    "\n",
    "    Examples:\n",
    "        >>> is_punct(\"!\")\n",
    "        True\n",
    "        >>> is_punct(\"Bonjour!\")\n",
    "        False\n",
    "        >>> is_punct(\"¿Te gusta el verano?\")\n",
    "        False\n",
    "        >>> is_punct(\"...\")\n",
    "        True\n",
    "        >>> is_punct(\"«»...\")\n",
    "        True\n",
    "\n",
    "    \"\"\"\n",
    "    return PUNCT_RE.match(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mother', 'died', 'today', 'Or', 'maybe', 'yesterday', 'I', 'ca', \"n't\", 'be', 'sure']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(corpus[2], language='english')\n",
    "\n",
    "# Loop with a standard for-loop\n",
    "tokenized = []\n",
    "for token in tokens:\n",
    "    if not is_punct(token):\n",
    "        tokenized.append(token)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language, lowercase=True):\n",
    "    \"\"\"Preprocess a text.\n",
    "\n",
    "    Perform a text preprocessing procedure, which transforms a string\n",
    "    object into a list of word tokens without punctuation markers.\n",
    "\n",
    "    Arguments:\n",
    "        text (str): a string representing a text.\n",
    "        language (str): a string specifying the language of text.\n",
    "        lowercase (bool, optional): Set to True to lowercase all\n",
    "            word tokens. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: a list of word tokens extracted from text, excluding\n",
    "            punctuation.\n",
    "\n",
    "    Examples:\n",
    "        >>> preprocess_text(\"Ah! Monsieur, c'est donc vous?\", 'french')\n",
    "        [\"ah\", \"monsieur\", \"c'est\", \"donc\", \"vous\"]\n",
    "\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Mother died today. Or maybe, yesterday; I can't be sure\n",
      "Tokenized: ['mother', 'died', 'today', 'or', 'maybe', 'yesterday', 'i', 'ca', \"n't\", 'be', 'sure']\n",
      "Original: It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn’t know what I was doing in New York\n",
      "Tokenized: ['it', 'was', 'a', 'queer', 'sultry', 'summer', 'the', 'summer', 'they', 'electrocuted', 'the', 'rosenbergs', 'and', 'i', 'didn', 't', 'know', 'what', 'i', 'was', 'doing', 'in', 'new', 'york']\n"
     ]
    }
   ],
   "source": [
    "for document in corpus[2:4]:\n",
    "    print('Original:', document)\n",
    "    print('Tokenized:', preprocess_text(document, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "vocabulary = collections.Counter()\n",
    "for document in corpus:\n",
    "    vocabulary.update(preprocess_text(document, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9), ('a', 6), ('i', 4), ('in', 4), ('was', 4)]\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary.most_common(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 100\n",
      "{'summer', 'the', 'on', 'as', 'of', 'i', 'and', 'a', 'man', 'was', 'it', 'in', 'were'}\n",
      "Pruned vocabulary size: 13\n"
     ]
    }
   ],
   "source": [
    "print('Original vocabulary size:', len(vocabulary))\n",
    "pruned_vocabulary = {token for token, count in vocabulary.items() if count > 1}\n",
    "print(pruned_vocabulary)\n",
    "print('Pruned vocabulary size:', len(pruned_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n",
    "    \"\"\"Extract a vocabulary from a tokenized corpus.\n",
    "\n",
    "    Arguments:\n",
    "        tokenized_corpus (list): a tokenized corpus represented, list\n",
    "            of lists of strings.\n",
    "        min_count (int, optional): the minimum occurrence count of a\n",
    "            vocabulary item in the corpus.\n",
    "        max_count (int, optional): the maximum occurrence count of a\n",
    "            vocabulary item in the corpus. Defaults to inf.\n",
    "\n",
    "    Returns:\n",
    "        list: An alphabetically ordered list of unique words in the\n",
    "            corpus, of which the frequencies adhere to the specified\n",
    "            minimum and maximum count.\n",
    "\n",
    "    Examples:\n",
    "        >>> corpus = [['the', 'man', 'love', 'man', 'the'],\n",
    "                      ['the', 'love', 'book', 'wise', 'drama'],\n",
    "                      ['a', 'story', 'book', 'drama']]\n",
    "        >>> extract_vocabulary(corpus, min_count=2)\n",
    "        ['book', 'drama', 'love', 'man', 'the']\n",
    "\n",
    "    \"\"\"\n",
    "    vocabulary = collections.Counter()\n",
    "    for document in tokenized_corpus:\n",
    "        vocabulary.update(document)\n",
    "    vocabulary = {word for word, count in vocabulary.items()\n",
    "                  if count >= min_count and count <= max_count}\n",
    "    return sorted(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [preprocess_text(document, 'english') for document in corpus]\n",
    "vocabulary = extract_vocabulary(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 3, 'we': 1, 'were': 1, 'somewhere': 1, 'around': 1, 'barstow': 1, 'on': 1, 'edge': 1, 'of': 1, 'desert': 1, 'when': 1, 'drugs': 1, 'began': 1, 'to': 1, 'take': 1, 'hold': 1})\n"
     ]
    }
   ],
   "source": [
    "bags_of_words = []\n",
    "for document in tokenized_corpus:\n",
    "    tokens = [word for word in document if word in vocabulary]\n",
    "    bags_of_words.append(collections.Counter(tokens))\n",
    "\n",
    "print(bags_of_words[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language, lowercase=True):\n",
    "    \"\"\"Preprocess a text.\n",
    "\n",
    "    Perform a text preprocessing procedure, which transforms a string\n",
    "    object into a list of word tokens without punctuation markers.\n",
    "\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\n",
    "    return tokens\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python388jvsc74a57bd081406bfb4a0df296520439dbeeb6cb234f05909e99c5846f7ab3626eb610afcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}